{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhhLwqfAp+0iUddPQX9Ftq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youssefawwad88/Tradingstation/blob/main/Trading_Station.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "#  MASTER SETUP CELL - Run this ONCE every time you start a new session\n",
        "# ===============================================================\n",
        "\n",
        "# 1. Connect to your permanent storage in Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Load your secret API key into the environment for our scripts to use\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['ALPHA_VANTAGE_API_KEY'] = userdata.get('ALPHA_VANTAGE_API_KEY')\n",
        "\n",
        "# 3. Install any Python libraries that Colab doesn't have by default\n",
        "!pip install schedule pytz pandas requests -q\n",
        "\n",
        "print(\"\\n✅ Environment setup complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ9ZyjaDzC-p",
        "outputId": "004431b4-2f84-4d5f-c0b9-7953310bb12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "✅ Environment setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Command: Execute the Data Fetcher\n",
        "This command runs the update_all_data.py script. The script's job is to read your tickerlist.txt file, then loop through each ticker and fetch the complete daily, 30-minute, and 1-minute data from the Alpha Vantage API. It then saves this data into organized CSV files within your Google Drive.\n",
        "\n",
        "What to Expect: You will see a series of print statements in the output as it processes each ticker and data type. This is the initial \"bulk download\" of your data."
      ],
      "metadata": {
        "id": "3la-zhyc-yad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/trading-system/jobs/update_all_data.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4gYCYOpxiQ_",
        "outputId": "1a90c6bb-dd41-41a2-9e7d-30f61eeb12a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 8 tickers from tickerlist.txt\n",
            "Verifying project directories...\n",
            "All directories are present in Google Drive.\n",
            "--- Starting Full Market Data Update ---\n",
            "Processing 8 tickers: ['NVDA', 'AAPL', 'TSLA', 'AMD', 'GOOGL', 'MSFT', 'AMZN', 'NFLX']\n",
            "\n",
            "--- Updating data for NVDA ---\n",
            "--> Fetching DAILY data for NVDA...\n",
            "    [✔] Successfully fetched 6472 rows of daily data for NVDA.\n",
            "    Merging data: Found 6471 existing rows in NVDA_daily.csv.\n",
            "    Merge complete: Added 1 new unique row(s). Total rows: 6472.\n",
            "    [✔] Successfully saved data for NVDA to NVDA_daily.csv\n",
            "--> Fetching REAL-TIME 30min data for NVDA...\n",
            "    [✔] Successfully fetched 666 rows for NVDA.\n",
            "    Merging data: Found 672 existing rows in NVDA_30min.csv.\n",
            "    Merge complete: Added 26 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for NVDA to NVDA_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for NVDA...\n",
            "    [✔] Successfully fetched 19980 rows for NVDA.\n",
            "    Merging data: Found 20134 existing rows in NVDA_1min.csv.\n",
            "    Merge complete: Added 806 new unique row(s). Total rows: 20940.\n",
            "    [✔] Successfully saved data for NVDA to NVDA_1min.csv\n",
            "\n",
            "--- Updating data for AAPL ---\n",
            "--> Fetching DAILY data for AAPL...\n",
            "    [✔] Successfully fetched 6472 rows of daily data for AAPL.\n",
            "    Merging data: Found 6471 existing rows in AAPL_daily.csv.\n",
            "    Merge complete: Added 1 new unique row(s). Total rows: 6472.\n",
            "    [✔] Successfully saved data for AAPL to AAPL_daily.csv\n",
            "--> Fetching REAL-TIME 30min data for AAPL...\n",
            "    [✔] Successfully fetched 666 rows for AAPL.\n",
            "    Merging data: Found 672 existing rows in AAPL_30min.csv.\n",
            "    Merge complete: Added 26 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for AAPL to AAPL_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for AAPL...\n",
            "    [✔] Successfully fetched 19943 rows for AAPL.\n",
            "    Merging data: Found 20053 existing rows in AAPL_1min.csv.\n",
            "    Merge complete: Added 850 new unique row(s). Total rows: 20903.\n",
            "    [✔] Successfully saved data for AAPL to AAPL_1min.csv\n",
            "\n",
            "--- Updating data for TSLA ---\n",
            "--> Fetching DAILY data for TSLA...\n",
            "    [✔] Successfully fetched 3792 rows of daily data for TSLA.\n",
            "    Merging data: Found 3791 existing rows in TSLA_daily.csv.\n",
            "    Merge complete: Added 1 new unique row(s). Total rows: 3792.\n",
            "    [✔] Successfully saved data for TSLA to TSLA_daily.csv\n",
            "--> Fetching REAL-TIME 30min data for TSLA...\n",
            "    [✔] Successfully fetched 666 rows for TSLA.\n",
            "    Merging data: Found 672 existing rows in TSLA_30min.csv.\n",
            "    Merge complete: Added 26 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for TSLA to TSLA_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for TSLA...\n",
            "    [✔] Successfully fetched 19980 rows for TSLA.\n",
            "    Merging data: Found 20143 existing rows in TSLA_1min.csv.\n",
            "    Merge complete: Added 797 new unique row(s). Total rows: 20940.\n",
            "    [✔] Successfully saved data for TSLA to TSLA_1min.csv\n",
            "\n",
            "--- Updating data for AMD ---\n",
            "--> Fetching DAILY data for AMD...\n",
            "    [✔] Successfully fetched 6472 rows of daily data for AMD.\n",
            "    Merging data: Found 6471 existing rows in AMD_daily.csv.\n",
            "    Merge complete: Added 1 new unique row(s). Total rows: 6472.\n",
            "    [✔] Successfully saved data for AMD to AMD_daily.csv\n",
            "--> Fetching REAL-TIME 30min data for AMD...\n",
            "    [✔] Successfully fetched 666 rows for AMD.\n",
            "    Merging data: Found 672 existing rows in AMD_30min.csv.\n",
            "    Merge complete: Added 26 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for AMD to AMD_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for AMD...\n",
            "    [✔] Successfully fetched 19957 rows for AMD.\n",
            "    Merging data: Found 20104 existing rows in AMD_1min.csv.\n",
            "    Merge complete: Added 813 new unique row(s). Total rows: 20917.\n",
            "    [✔] Successfully saved data for AMD to AMD_1min.csv\n",
            "\n",
            "--- Updating data for GOOGL ---\n",
            "--> Fetching DAILY data for GOOGL...\n",
            "    [✔] Successfully fetched 5267 rows of daily data for GOOGL.\n",
            "    Merging data: Found 5266 existing rows in GOOGL_daily.csv.\n",
            "    Merge complete: Added 1 new unique row(s). Total rows: 5267.\n",
            "    [✔] Successfully saved data for GOOGL to GOOGL_daily.csv\n",
            "--> Fetching REAL-TIME 30min data for GOOGL...\n",
            "    [✔] Successfully fetched 666 rows for GOOGL.\n",
            "    Merging data: Found 672 existing rows in GOOGL_30min.csv.\n",
            "    Merge complete: Added 26 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for GOOGL to GOOGL_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for GOOGL...\n",
            "    [✔] Successfully fetched 19913 rows for GOOGL.\n",
            "    Merging data: Found 20055 existing rows in GOOGL_1min.csv.\n",
            "    Merge complete: Added 817 new unique row(s). Total rows: 20872.\n",
            "    [✔] Successfully saved data for GOOGL to GOOGL_1min.csv\n",
            "\n",
            "--- Updating data for MSFT ---\n",
            "--> Fetching DAILY data for MSFT...\n",
            "    [✔] Successfully fetched 6472 rows of daily data for MSFT.\n",
            "    Merging data: Found 6471 existing rows in MSFT_daily.csv.\n",
            "    Merge complete: Added 1 new unique row(s). Total rows: 6472.\n",
            "    [✔] Successfully saved data for MSFT to MSFT_daily.csv\n",
            "--> Fetching REAL-TIME 30min data for MSFT...\n",
            "    [✔] Successfully fetched 666 rows for MSFT.\n",
            "    Merging data: Found 672 existing rows in MSFT_30min.csv.\n",
            "    Merge complete: Added 26 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for MSFT to MSFT_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for MSFT...\n",
            "    [✔] Successfully fetched 19553 rows for MSFT.\n",
            "    Merging data: Found 19644 existing rows in MSFT_1min.csv.\n",
            "    Merge complete: Added 858 new unique row(s). Total rows: 20502.\n",
            "    [✔] Successfully saved data for MSFT to MSFT_1min.csv\n",
            "\n",
            "--- Updating data for AMZN ---\n",
            "--> Fetching DAILY data for AMZN...\n",
            "    [✔] Successfully fetched 6472 rows of daily data for AMZN.\n",
            "    Merging data: Found 6471 existing rows in AMZN_daily.csv.\n",
            "    Merge complete: Added 1 new unique row(s). Total rows: 6472.\n",
            "    [✔] Successfully saved data for AMZN to AMZN_daily.csv\n",
            "--> Fetching REAL-TIME 30min data for AMZN...\n",
            "    [✔] Successfully fetched 666 rows for AMZN.\n",
            "    Merging data: Found 672 existing rows in AMZN_30min.csv.\n",
            "    Merge complete: Added 26 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for AMZN to AMZN_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for AMZN...\n",
            "    [✔] Successfully fetched 19794 rows for AMZN.\n",
            "    Merging data: Found 19908 existing rows in AMZN_1min.csv.\n",
            "    Merge complete: Added 844 new unique row(s). Total rows: 20752.\n",
            "    [✔] Successfully saved data for AMZN to AMZN_1min.csv\n",
            "\n",
            "--- Updating data for NFLX ---\n",
            "--> Fetching DAILY data for NFLX...\n",
            "    [✔] Successfully fetched 5831 rows of daily data for NFLX.\n",
            "    Merging data: Found 5830 existing rows in NFLX_daily.csv.\n",
            "    Merge complete: Added 1 new unique row(s). Total rows: 5831.\n",
            "    [✔] Successfully saved data for NFLX to NFLX_daily.csv\n",
            "--> Fetching REAL-TIME 30min data for NFLX...\n",
            "    [✔] Successfully fetched 666 rows for NFLX.\n",
            "    Merging data: Found 672 existing rows in NFLX_30min.csv.\n",
            "    Merge complete: Added 26 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for NFLX to NFLX_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for NFLX...\n",
            "    [✔] Successfully fetched 18638 rows for NFLX.\n",
            "    Merging data: Found 18664 existing rows in NFLX_1min.csv.\n",
            "    Merge complete: Added 823 new unique row(s). Total rows: 19487.\n",
            "    [✔] Successfully saved data for NFLX to NFLX_1min.csv\n",
            "\n",
            "--- Full Market Data Update Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Command: Execute the Compact Intraday Updater**\n",
        "This command runs the update_intraday_compact.py script. This script is optimized for frequent, fast updates. It only downloads the last 100 data points from the API and intelligently appends the newest 1-minute and 30-minute candles to your existing CSV files.\n",
        "\n",
        "When to Use: This is the command you will use repeatedly throughout the trading day to keep your intraday data live."
      ],
      "metadata": {
        "id": "48L84_9zRGQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/trading-system/jobs/update_intraday_compact.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4BX9tc-RJRX",
        "outputId": "322f87b3-372f-4067-b834-85f44070d6a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 8 tickers from tickerlist.txt\n",
            "Verifying project directories...\n",
            "All directories are present in Google Drive.\n",
            "--- Starting Compact Intraday Data Update ---\n",
            "Processing 8 tickers for compact update...\n",
            "\n",
            "--- Updating intraday data for NVDA ---\n",
            "--> Fetching REAL-TIME 30min data for NVDA...\n",
            "    [✔] Successfully fetched 100 rows for NVDA.\n",
            "    Merging data: Found 698 existing rows in NVDA_30min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for NVDA to NVDA_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for NVDA...\n",
            "    [✔] Successfully fetched 100 rows for NVDA.\n",
            "    Merging data: Found 20940 existing rows in NVDA_1min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 20940.\n",
            "    [✔] Successfully saved data for NVDA to NVDA_1min.csv\n",
            "\n",
            "--- Updating intraday data for AAPL ---\n",
            "--> Fetching REAL-TIME 30min data for AAPL...\n",
            "    [✔] Successfully fetched 100 rows for AAPL.\n",
            "    Merging data: Found 698 existing rows in AAPL_30min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for AAPL to AAPL_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for AAPL...\n",
            "    [✔] Successfully fetched 100 rows for AAPL.\n",
            "    Merging data: Found 20903 existing rows in AAPL_1min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 20903.\n",
            "    [✔] Successfully saved data for AAPL to AAPL_1min.csv\n",
            "\n",
            "--- Updating intraday data for TSLA ---\n",
            "--> Fetching REAL-TIME 30min data for TSLA...\n",
            "    [✔] Successfully fetched 100 rows for TSLA.\n",
            "    Merging data: Found 698 existing rows in TSLA_30min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for TSLA to TSLA_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for TSLA...\n",
            "    [✔] Successfully fetched 100 rows for TSLA.\n",
            "    Merging data: Found 20940 existing rows in TSLA_1min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 20940.\n",
            "    [✔] Successfully saved data for TSLA to TSLA_1min.csv\n",
            "\n",
            "--- Updating intraday data for AMD ---\n",
            "--> Fetching REAL-TIME 30min data for AMD...\n",
            "    [✔] Successfully fetched 100 rows for AMD.\n",
            "    Merging data: Found 698 existing rows in AMD_30min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for AMD to AMD_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for AMD...\n",
            "    [✔] Successfully fetched 100 rows for AMD.\n",
            "    Merging data: Found 20917 existing rows in AMD_1min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 20917.\n",
            "    [✔] Successfully saved data for AMD to AMD_1min.csv\n",
            "\n",
            "--- Updating intraday data for GOOGL ---\n",
            "--> Fetching REAL-TIME 30min data for GOOGL...\n",
            "    [✔] Successfully fetched 100 rows for GOOGL.\n",
            "    Merging data: Found 698 existing rows in GOOGL_30min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for GOOGL to GOOGL_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for GOOGL...\n",
            "    [✔] Successfully fetched 100 rows for GOOGL.\n",
            "    Merging data: Found 20872 existing rows in GOOGL_1min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 20872.\n",
            "    [✔] Successfully saved data for GOOGL to GOOGL_1min.csv\n",
            "\n",
            "--- Updating intraday data for MSFT ---\n",
            "--> Fetching REAL-TIME 30min data for MSFT...\n",
            "    [✔] Successfully fetched 100 rows for MSFT.\n",
            "    Merging data: Found 698 existing rows in MSFT_30min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for MSFT to MSFT_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for MSFT...\n",
            "    [✔] Successfully fetched 100 rows for MSFT.\n",
            "    Merging data: Found 20502 existing rows in MSFT_1min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 20502.\n",
            "    [✔] Successfully saved data for MSFT to MSFT_1min.csv\n",
            "\n",
            "--- Updating intraday data for AMZN ---\n",
            "--> Fetching REAL-TIME 30min data for AMZN...\n",
            "    [✔] Successfully fetched 100 rows for AMZN.\n",
            "    Merging data: Found 698 existing rows in AMZN_30min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for AMZN to AMZN_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for AMZN...\n",
            "    [✔] Successfully fetched 100 rows for AMZN.\n",
            "    Merging data: Found 20752 existing rows in AMZN_1min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 20752.\n",
            "    [✔] Successfully saved data for AMZN to AMZN_1min.csv\n",
            "\n",
            "--- Updating intraday data for NFLX ---\n",
            "--> Fetching REAL-TIME 30min data for NFLX...\n",
            "    [✔] Successfully fetched 100 rows for NFLX.\n",
            "    Merging data: Found 698 existing rows in NFLX_30min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 698.\n",
            "    [✔] Successfully saved data for NFLX to NFLX_30min.csv\n",
            "--> Fetching REAL-TIME 1min data for NFLX...\n",
            "    [✔] Successfully fetched 100 rows for NFLX.\n",
            "    Merging data: Found 19487 existing rows in NFLX_1min.csv.\n",
            "    Merge complete: Added 0 new unique row(s). Total rows: 19487.\n",
            "    [✔] Successfully saved data for NFLX to NFLX_1min.csv\n",
            "\n",
            "--- Compact Intraday Data Update Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gap & Go Screener\n",
        "This command will run the gapgo.py script we just created. The script will load the data for each of your tickers, apply the Gap & Go trading logic, and print a message if it finds any valid setups. If any signals are found, it will save them to a new file: /data/signals/gapgo_signals.csv.\n",
        "\n",
        "What to Expect: The script will print its progress for each ticker. If it's outside of market hours, it will likely skip most tickers because there is no fresh pre-market or opening data."
      ],
      "metadata": {
        "id": "X_Eaus_icBtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/trading-system/screeners/gapgo.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_12vrv07cIxm",
        "outputId": "04859472-bc3f-424a-dc1e-6740831dd4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 8 tickers from tickerlist.txt\n",
            "Verifying project directories...\n",
            "All directories are present in Google Drive.\n",
            "\n",
            "--- Running Gap & Go Screener (Execution-Grade) ---\n",
            "Current Market Session Detected: PREMARKET\n",
            "Saving 8 signal(s) from gapgo to /content/drive/MyDrive/trading-system/data/signals/gapgo_signals.csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10-Minute ORB Screener\n",
        "Description: This command runs the orb.py script. The script identifies stocks that break out of their first 10-minute opening range (9:30 - 9:40 AM EST). For best results, run this command at or after 9:41 AM EST. It will save any identified breakouts or breakdowns to /data/signals/orb_signals.csv."
      ],
      "metadata": {
        "id": "QmPN0Gp_isQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/trading-system/screeners/orb.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yigT2ePviumO",
        "outputId": "eea9b39b-6910-42cd-c4c1-9dabc6f597d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 8 tickers from tickerlist.txt\n",
            "Verifying project directories...\n",
            "All directories are present in Google Drive.\n",
            "\n",
            "--- Running Opening Range Breakout (ORB) Screener ---\n",
            "Market session is PREMARKET. ORB screener only runs during REGULAR session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find AVWAP Anchor Points\n",
        "\n",
        "This command runs the find_avwap_anchors.py job. It analyzes daily data to find significant \"power candles\" and saves them as anchor dates to /data/avwap_anchors.csv. Run this once before starting the main screener."
      ],
      "metadata": {
        "id": "79iwF2xPrOfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/trading-system/jobs/find_avwap_anchors.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90fO9y0TrR-P",
        "outputId": "9304c150-b2eb-48ca-81e5-07976b6c6948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 8 tickers from tickerlist.txt\n",
            "Verifying project directories...\n",
            "All directories are present in Google Drive.\n",
            "\n",
            "--- Running AVWAP Anchor Finder Job ---\n",
            "Successfully saved 8 detailed anchor sets to /content/drive/MyDrive/trading-system/data/avwap_anchors.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anchored VWAP Screener\n",
        "\n",
        "This command runs the avwap.py screener. It reads the pre-calculated anchor dates and checks for real-time reclaims or rejections of the AVWAP levels on high volume. It saves the results to /data/signals/avwap_signals.csv."
      ],
      "metadata": {
        "id": "owdAuAnWpzqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/trading-system/screeners/avwap.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdZ8fRW5p3oO",
        "outputId": "e3e9c0eb-5b55-4a84-f7f9-75125107080e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 8 tickers from tickerlist.txt\n",
            "Verifying project directories...\n",
            "All directories are present in Google Drive.\n",
            "\n",
            "--- Running Anchored VWAP (AVWAP) Screener ---\n",
            "Saving 8 signal(s) from avwap to /content/drive/MyDrive/trading-system/data/signals/avwap_signals.csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breakout Screener\n",
        "Description: This command runs the breakout.py script. It analyzes daily data to find stocks that have closed above their upper Bollinger Band on high volume (at least 150% of the 20-day average). It saves any valid signals to /data/signals/breakout_signals.csv."
      ],
      "metadata": {
        "id": "HsWCAq3f2_rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/trading-system/screeners/breakout.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfurvWF72_Fu",
        "outputId": "a2f92a71-0fa5-4ce2-d28e-cfdec84ecfd4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 8 tickers from tickerlist.txt\n",
            "Verifying project directories...\n",
            "All directories are present in Google Drive.\n",
            "\n",
            "--- Running Daily Breakout & Breakdown Screener (Advanced) ---\n",
            "   - INFO: No confirmed anchor date found for AAPL. AVWAP check will default to 'No'.\n",
            "   - INFO: No confirmed anchor date found for MSFT. AVWAP check will default to 'No'.\n",
            "   - INFO: No confirmed anchor date found for AMZN. AVWAP check will default to 'No'.\n",
            "Saving 8 signal(s) from breakout to /content/drive/MyDrive/trading-system/data/signals/breakout_signals.csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMA Trend Pullback Screener\n",
        "\n",
        "Description: This command runs the ema_pullback.py script. It analyzes daily data to find stocks in a clear trend (above/below the 50 EMA) that have pulled back to the 21 EMA and then printed a strong confirmation candle on high volume. It saves any valid signals to /data/signals/ema_pullback_signals.csv."
      ],
      "metadata": {
        "id": "3PxQigroHtAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/trading-system/screeners/ema_pullback.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btMb_S5DHy1s",
        "outputId": "8a109840-9d2c-4780-f522-470af8c4d4d9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 8 tickers from tickerlist.txt\n",
            "Verifying project directories...\n",
            "All directories are present in Google Drive.\n",
            "\n",
            "--- Running EMA Trend Pullback Screener (with advanced columns) ---\n",
            "Saving 8 signal(s) from ema_pullback to /content/drive/MyDrive/trading-system/data/signals/ema_pullback_signals.csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B6DMxh1cH0ys"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}